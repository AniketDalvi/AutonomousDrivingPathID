<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLUXza5VhXqVC6o75Eld_V98');.lst-kix_list_1-3>li:before{content:"\0025aa  "}.lst-kix_list_1-4>li:before{content:"\0025aa  "}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_1-7>li:before{content:"\0025aa  "}.lst-kix_list_1-5>li:before{content:"\0025aa  "}.lst-kix_list_1-6>li:before{content:"\0025aa  "}ul.lst-kix_list_1-3{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf  "}ul.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_1-8>li:before{content:"\0025aa  "}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_1-1>li:before{content:"o  "}.lst-kix_list_1-2>li:before{content:"\0025aa  "}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c12{background-color:#ffffff;color:#1c4587;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:17pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Calibri";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c5{padding-top:0pt;padding-bottom:8pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:justify}.c9{padding-top:0pt;padding-bottom:8pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c21{padding-top:0pt;padding-bottom:8pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c15{-webkit-text-decoration-skip:none;font-weight:700;text-decoration:underline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Arial"}.c22{background-color:#ffffff;font-size:12pt;font-family:"Times New Roman";color:#333333;font-weight:400}.c17{font-size:12pt;font-family:"Times New Roman";font-style:italic;color:#333333;font-weight:400}.c16{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c25{font-size:12pt;font-family:"Arial";color:#000000;font-weight:700}.c3{font-size:12pt;font-family:"Arial";color:#000000;font-weight:400}.c8{font-size:12pt;font-family:"Arial";font-weight:400}.c20{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c24{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c19{color:#000000;vertical-align:baseline;font-style:normal}.c26{margin-left:36pt;text-indent:-36pt}.c23{color:#1c4587;font-size:17pt}.c4{color:#073763;font-size:17pt}.c27{font-weight:700;font-family:"Arial"}.c7{height:11pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0791666666666666;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Calibri"}p{margin:0;color:#000000;font-size:11pt;font-family:"Calibri"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c20"><div><p class="c7 c9"><span class="c14"></span></p></div><p class="c5"><span class="c18">Road Detection for Autonomous Navigation in Self Driving Cars</span></p><p class="c5"><span class="c13">Aniket Dalvi, Aman Lunia, Tushar Narang, Yash Shah</span></p><p class="c2"><span class="c12">Purpose &amp; Background Information:</span></p><p class="c2 c7"><span class="c13"></span></p><p class="c2"><span class="c3">Self-driving cars have many important applications and benefits ranging from long-term improvement in passenger safety, reduction in accident rates to making road traffic regulation less challenging. Due to their lack of dependence on individuals, self-driving cars reduce the chance for human error and make driving efficient on multiple fronts. As a result autonomous driving has become a major area of research both in academia and industry. While autonomous cars may not be completely in production yet, assisted driving is provided with most if not all cars sold today in which computer vision plays a huge role.</span></p><p class="c21"><span class="c24"><br></span><span class="c3">Identifying ideal terrain and correct lanes for driving is an essential element of any autonomous vehicle vision system. For our project we have chosen to focus on </span><span class="c25">identifying ideal drivable surfaces for autonomous driving vision systems</span><span class="c3">. Given the view ahead of a vehicle, we attempt to determine sections of the view that are ideal for driving. We start out by classifying roads and lanes from static pictures, only to further enhance the algorithm to efficiently work for continuous feeds of frames. Finally, we will further add capabilities to understand and navigate on unstructured roads or under non-ideal weather conditions such as heavy fog and snowfall.</span><span>&nbsp;&nbsp;</span></p><p class="c2"><span class="c16 c23">Approach and Implementation:</span></p><p class="c2 c7"><span class="c13"></span></p><p class="c2"><span class="c1">Our source of inspiration came from the paper, Robust Unstructured Road Detection: The Importance of Contextual Information (Shang et.al, 2012), which is published in the International Journal of Advanced Robotics Systems. </span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c1">We start by dividing the image into windows of size 10x10, where each of these fixed windows (units) are converted to a high dimensional vector, which we plan to classify as &lsquo;road&rsquo; or &lsquo;not road.&rsquo; The feature vector for each individual unit of the image is created by generating it&rsquo;s corresponding RGB histogram where we chose the number of bins to be 24 giving red, green and blue each 8 bins.</span></p><p class="c6"><span class="c1">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.50px; height: 62.25px;"><img alt="" src="images/image8.png" style="width: 626.50px; height: 62.25px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c1">This 24 dimensional feature vector was then passed into two machine learning algorithms being support-vector-machines (SVM) and k-nearest-neighbors (KNN) where it learnt a representation for a surface that could and couldn&rsquo;t be driven on.</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c15 c19">Support Vector Machines</span></p><p class="c2"><span class="c8">Support Vector Machines</span><span class="c1">&nbsp;simply separate data points according to class by learning an equation of a function that maximizes the distance between them.</span></p><p class="c2"><span class="c1">For this project, a linear kernel was chosen along with a soft margin (c-factor) of 0.25. These hyperparameters were chosen to mainly prevent overfitting and because of a linear model&rsquo;s ability to generalize well while keeping the representation simple and easy to visualize. Another kernel we considered was the RBF/Gaussian kernel. While it yielded slightly better results, it was significantly slower to its linear counterpart. The results we obtained from the SVM are as follows:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 585.04px; height: 206.93px;"><img alt="https://lh4.googleusercontent.com/RMWKX_v1m--7Q_p_Vh_KbC50AWH30ow2Z4NDpF0r6WL5yjYPq1ml7LG_vxEI5LWR5WsvMbr6fPLhZU7ogUyZnKaWaQTUsYwaOTNwRvlK2fpL8BXWPP3RkC-Y9dmg5VzpTTqJN28vaE4" src="images/image4.png" style="width: 585.04px; height: 206.93px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c13">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 583.50px; height: 219.22px;"><img alt="https://lh4.googleusercontent.com/eOutvOxfk3wkn8Kcw3fQljHC9rWrHcEst7cbC1a1yuZuydMypE34rtPB1j1JlC_bw4gAC5hPd9gH0TdTkkaSYj-U0o669KNo4e00H01UWUUcsWnvr44zq0b3H3OyXtaQrB-y-RtbDGA" src="images/image7.png" style="width: 583.50px; height: 219.22px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c7"><span class="c1"></span></p><p class="c0"><span class="c1">While it identified a good portion of the road, it&rsquo;s results weren&rsquo;t satisfactory. </span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c15 c19">K-Nearest Neighbors</span></p><p class="c0"><span class="c8">The KNN algorithm</span><span class="c1">&nbsp;works by finding the k closest training instances to the given test instance and predicts the label belonging to most of these instances. &nbsp;</span></p><p class="c0"><span class="c1">After testing with different k values, we found the algorithm to be the most successful when k was chosen as 20. These were the initial results obtained:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 590.04px; height: 178.16px;"><img alt="https://lh6.googleusercontent.com/ZeIXg3E3Nc7X5FPd8_fDg3Ky-_6r-chEOkibspr8dMcbpNhvWs6y781ND2gVjtyv-8NfNBcazNP-xXb1EBqAp_Lum6YwJWAUa0FNCLHpqAIAMJJ5LzZk9VWlp9LU5srAtmstjJoRYp4" src="images/image5.jpg" style="width: 590.04px; height: 178.16px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 591.50px; height: 178.95px;"><img alt="https://lh3.googleusercontent.com/cQpIwTWZo_oCiZGQv2yLA12Yj_VNQwfzthPpbZtpQdVto5nIto-SqHsV8KBoSDjjT-JgRqDb0_GOCi2yY-5Iu04YKdL3SNBTXcV3UECRC3iQP34XNor1KdsZZzANgQqg-IEYMXpJ7uM" src="images/image11.jpg" style="width: 591.50px; height: 178.95px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c1">Switching to a weighted neighbors approach improved our findings, </span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 582.83px; height: 175.84px;"><img alt="https://lh3.googleusercontent.com/s1AFcR_1qEvT4eTUIrrHCJeY07eVhy2WnqlN8QEkFs5NZwk7wTes1vboAXlXgMMl2F7nqL1SaqGpMGIdInonx49groIjwGPlbxtklalBFS85TZWD3-blNEfZK81bzR6Bh1iLis3hdl8" src="images/image9.jpg" style="width: 582.83px; height: 175.84px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 584.88px; height: 176.46px;"><img alt="https://lh3.googleusercontent.com/5pHT5IE0r49yyxjXhb6qZ5ps5w9xREZ0d8pyruIC13hcXv7kgwEKLQh42anE-K_5cv73BYhx4508vhjbapgbbB1k1SJPQbewH8o_P7mBZeFP8C6xc68VhGQSZzAP5SZdE7_6HvfQSGA" src="images/image14.jpg" style="width: 584.88px; height: 176.46px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c3">The KNN yielded better results than our SVM and this finding </span><span class="c8">a</span><span class="c1">ffects our later choices.</span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c15">Combination (</span><span class="c15 c19">KNN &amp; SVM):</span></p><p class="c0"><span class="c1">To further improve results the paper combined the two classifiers and a confidence map, which was developed using GPS data from the car and road image maps. Due to a lack of car GPS information we initially focused on just the two machine learning algorithms. </span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 435.57px; height: 132.56px;"><img alt="https://lh3.googleusercontent.com/ScK6ziVR9_lyOIPudlTttT7JKlho6CzlOfOEaXsO8FHPwtW36huMzqyrUG_NHMQzvXi5UDyOI6haE7J7nvmuZwlgssOlJc0d1L7UNTc3FzVToB4bxtwzModClrAXfJ-s_nv9BEOowtQ" src="images/image13.png" style="width: 435.57px; height: 132.56px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c27">Bayesian Framework used by the paper to combine SVM, KNN and Confidence Map</span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c1">The confidence of the classification decision made by both algorithms weigh in the final decision. Since our KNN performed better in most cases, it was weighted higher.</span></p><p class="c0"><span class="c1">The results obtained from this combination can be seen as below,</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 600.21px; height: 182.03px;"><img alt="https://lh4.googleusercontent.com/YMrfhWG9MjYpSbKHi8HmG-WN3nSNxD9snN6s0NgA-go--RJb1ZSSHbd1PPSyo5vZmU1CGX-1MElJvPbKSeL_icGtrmkfh_Irzs3oC9VqZLEugsDKAakemwSIWRsJ7HP905xt0bGfO94" src="images/image16.png" style="width: 600.21px; height: 182.03px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.88px; height: 183.45px;"><img alt="https://lh6.googleusercontent.com/Ghem5w_nq5FsoG2kMqdZ5TG-NMuvhE6QtGJPBaQkraPyq-4hJ02nRUnmE3mLVvT-qtsngVLaofiZsFa1hheGjnitJJYsiO-fhUKaz1sVkv3_8y7UtPtUgR5Ndsr1jghx-H9CFzf6rqI" src="images/image15.png" style="width: 604.88px; height: 183.45px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">This did improve our overall performance and as you can see the results look similar to those produced by the KNN, which is due to the fact that it was weighted more. </span></p><p class="c0"><span>&nbsp;</span></p><p class="c0"><span class="c4">&nbsp;</span><span class="c16 c4">Experimentation</span></p><p class="c0"><span class="c1">After implementing the paper, we noticed that the accuracy of our results was not up to our expectation. We suspected that this due to the lack of a &ldquo;confidence map&rdquo; to go with our SVM and KNN, as the results in the paper significantly improve on the inclusion of the &ldquo;confidence map&rdquo;. We realized that due to lack of resources like GPS data and road maps we could not implement the confidence map in the manner the paper described. We thus tried various experimentation techniques to build our version of a confidence map to improve our results. </span></p><p class="c0"><span class="c8">First, we attempted to create a binary representation of the road image using computer vision techniques learnt in class like thresholding and morphological operations of erosion and dilation. Using this, we aimed to get a binary image such that the road would be white and the environment around it black. This would allow us to eliminate false positives which were detected outside the drivable region. However creating an accurate binary representation was quite challenging due to the presence of multiple objects in the image. Some of our results from this attempt can be seen here:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 162.67px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 162.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 2.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 158.50px;"><img alt="https://lh5.googleusercontent.com/v0h_TkHfYf_cshsZJ2uR6u82nebfZ3RzwFoFdOPRN7d2wCRZCVfN7m1HF23sSN76aLDl7Vc3h2FzGV5RGsVqDMmjzHioCmymw-q4LqLZVFUHZg1kTLULHcU3wZO5qJhwyms3" src="images/image19.png" style="width: 624.00px; height: 158.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">As can be seen above, even after multiple attempts with changing the erosion, dilation and thresholding parameters, we were not able to get satisfactory results with this approach and decided not to pursue it further, but instead started brainstorming for other ideas. </span></p><p class="c0"><span class="c8">Soon, we decided to experiment with another approach involving the vanishing point of the image. We observed that in most cases the drivable region fell below vanishing point of the image. We thus, proceeded to find the vanishing point of our road images, and consider only pixels that fell below it for detecting roads. This approach worked well for ideal road condition as can be seen below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.07px; height: 269.13px;"><img alt="https://lh3.googleusercontent.com/iHz5dhNDzpn1a9GC69GNKU7ajbZihvsSuVi370C6ectmN-r3iwZDinTLjy1DazHmRlhcJmfcYQ1nGFlerSyuCjZlh7fmOXN5Y0bzK-jC9zEH0pwgoZ1gVYnPK4zgmltfOYgb" src="images/image21.png" style="width: 504.07px; height: 269.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">However, in real world road environments, the drivable region was not always a straight road, and such cases resulted in erroneous detection of the vanishing point as can be seen in the images below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 567.50px; height: 171.89px;"><img alt="" src="images/image18.jpg" style="width: 567.50px; height: 171.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 567.87px; height: 172.00px;"><img alt="" src="images/image22.png" style="width: 567.87px; height: 172.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">After getting the above unsatisfactory results, we decided to not continue this approach as well. However, we still wanted to pursue with the general idea of using locational information in addition to the RGB values to improve results. We thus decided to update our feature vector to include the X and Y coordinate values of our pixels windows as well. This change in the feature gave us relatively better results compared to our initial results other experimentation techniques. Some of the results from this approach can be seen below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 732.07px; height: 221.20px;"><img alt="https://lh3.googleusercontent.com/nRco1xcsiNsfgMP2QGJDFwa6p-iMi2COvh3_mrSFxgEzjH28qnmTFFYNzxlx7h42rYAyCKQlL5Nt26CmdDzYq9wrdEP-FYJzRwAHYRUXgE85WFinggoFHXGUzOXUImYr9a80" src="images/image20.png" style="width: 732.07px; height: 221.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 730.00px; height: 220.47px;"><img alt="https://lh6.googleusercontent.com/ahf-VezX-0n8WbcTQdwr-Y3Q3Zkb3k9I6bpzQH8u2jO9if7Vixt26tTgLBxf6Jczcy1l358gPCTeLRIkhn_IdXK-WPoLgiM14ZptWyEDLgTKCUPAygxl9SjGjBk8Nu4Qaxgx" src="images/image1.png" style="width: 730.00px; height: 220.47px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 728.93px; height: 220.79px;"><img alt="https://lh5.googleusercontent.com/UxYe--66TgnmwR3iR0MlU0IoQeUSDIHA3NtIWiLk4bBBuzd6uUvezqKIisZkpuen0oNmsv62gdFKbwbgMzCranfwPEhPfaL4le_ok-yYjNuAxQiYXnUuHm0sdIqbQUyLqdZt" src="images/image3.png" style="width: 728.93px; height: 220.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c1">Looking at this improvement in results, we decided to further use locational data and computer vision techniques learnt over the course of the semester to further reduce false positives and increase true positives. To do this we used edge detection techniques to find all the edges in the image, and then used trigonometry to decide the edges which most closely represented the road. This is because we noticed that the edges that represented the road had the smallest vertically opposite angles. After doing this computation and considering only pixel windows that fell in between these edges and running a combination of SVM and KNN on them with the modified feature vector, we got our final results which were pretty impressive as compared to any of our previous techniques. These results have been shown below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 715.73px; height: 216.27px;"><img alt="https://lh6.googleusercontent.com/DqHezwQTB6malpEqpyg3Byio9VVW8jFzpmMAby6aowcgaLoAHeqtTzxubRRHOBm8v-AWseZvjlcY_uk9UprKIzTWnZ0Ty2jgnxbCY_dxC5cXfDImN1bc86vCoRIoVaYqXQOD" src="images/image10.png" style="width: 715.73px; height: 216.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 714.07px; height: 216.27px;"><img alt="https://lh4.googleusercontent.com/m9HVAV6OvBNPHfH4qpj0sR8da3f93XSTmnaxIvFoaHiNA1Sy9qGH8A2EOV08ebFEcs93x1rW2kZin4R-E6Q8RJfw0VUXilGX7yjPMuLAFvrJL9guJUovGCQ7qKgudGcsF6gr" src="images/image12.png" style="width: 714.07px; height: 216.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c1">Thus, after multiple approaches we got our best and most compelling results by expanding the feature vector to incorporate locational information and running the combined SVM and KNN algorithm on the pixels between the most probable road edges.</span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c16 c4">Future Work</span></p><p class="c0"><span class="c1">In the future, we plan to continue working on this project and improve its accuracy using fewer training instances. This is because we noticed that our approach currently doesn&rsquo;t work well in cases where the road is too bright due to excessive sunlight or too dark due to shadows. A few examples of faulty results can be seen below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 715.73px; height: 216.27px;"><img alt="https://lh4.googleusercontent.com/jx_TH2BRnDawahBI-0wSqb_u0bGnMHAkZPRi4Te28FmGIem5Chb9tfq6F-s1WamgQZag8UCM3Fdq9bEnlBYAc5taqdVOI147ABwye8L-gbgdsiOikJt3lv_61-S0PWLK1bQo" src="images/image17.png" style="width: 715.73px; height: 216.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 713.93px; height: 216.27px;"><img alt="https://lh4.googleusercontent.com/a00jQCql5bIYlDd50-T-oJr-FfmnPXiD8QBqb7aJaJ8G6EGRTpzQA-AvR7FVap8NerJ_mb_ziHtB6ix1ky57EBiQ2S1-NMfYOK7RPwWGltdYtFRF0lV1jeyIxrNIxPWwi9tM" src="images/image2.png" style="width: 713.93px; height: 216.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0" id="h.gjdgxs"><span class="c1">&nbsp;We also observed that there is a massive scope to refine the code to optimize it and make it run faster such that it can be used on real time videos. Apart from this, we also want to come up with a way to test it against other approaches and thereby arrive at an algorithm that would give us a quantitative measurement of the accuracy of our approach. Finally, considering how much the presence of open source code and documentation aided us in our project, we would like to clean up our code and present it in a format that can be made open source so that other students like us can benefit from it.</span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0"><span class="c4 c16">Learning Experience</span></p><p class="c0"><span class="c1">This project was an extremely enriching learning experience. Firstly, most of us had never worked on a project like this which involved coming up with our own idea, researching resources available to implement it, implementing it from scratch without a skeleton to follow, testing out approaches, brainstorming novel ideas to improve results and arriving at a solution after multiple experimentation approaches. We, thus, got a taste of what it feels like to be involved in a non-school, real world project. Furthermore, on the technical side, none of us had worked with SVMs or KNNs before, and most of us weren&rsquo;t proficient with python either. However, after working on this project for a couple of months, we got a chance to work on these in depth and get sufficiently acquainted with them, an opportunity we wouldn&rsquo;t have otherwise gotten. Moreover, this being a team project, we also picked up crucial team management skills and learnt the version control techniques using git which makes collaboration easier. All in all, it was a great educational experience, and we would like to thank professor Mohit Gupta for presenting us with such an opportunity.</span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0 c7"><span class="c1"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c0 c7"><span class="c10"></span></p><p class="c6"><span class="c10">Bibliography</span></p><p class="c6 c7"><span class="c10"></span></p><p class="c2 c26"><span class="c22">Shang, E., An, X., Li, J., Ye, L., &amp; He, H. (2013). Robust Unstructured Road Detection: The Importance of Contextual Information. </span><span class="c17">International Journal of Advanced Robotic Systems,10</span><span class="c22">(3), 179. doi:10.5772/55560</span></p></body></html>